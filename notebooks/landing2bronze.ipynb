{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da50cbd-908c-423d-81a6-79c5708606f3",
   "metadata": {},
   "source": [
    "## Ingestão de Dados: Meios de Pagamentos\n",
    "\n",
    "Vamos desenvolver toda a lógica da extração dos dados disponibilizados na camada Landing, manipulando e gravando no Delta Lake entre as camadas Bronze, Silver e Gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715d8b98-4fa9-475b-af23-9161fb436ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTS AND LIBRARIES\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configuração do logger\n",
    "logger = logging.getLogger(\"minio_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Configurando o formato do log\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "# PySpark Libraries\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, explode, lit, from_json\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa25188-fea7-43b2-905d-c0102a1bfc21",
   "metadata": {},
   "source": [
    "### 02. Criando variáveis globais e configurando Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8776bef4-c10e-4d9d-86cf-3e3e4fb06aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis Globais e de Ambiente para o Projeto.\n",
    "\n",
    "os.environ[\"MINIO_KEY\"] = \"developer\"\n",
    "os.environ[\"MINIO_SECRET\"] = \"developer01\"\n",
    "os.environ[\"MINIO_ENDPOINT\"] = \"http://minio:9000\"\n",
    "\n",
    "\n",
    "# Na ausência do dbtuils ou do Metastore vou usar o boto para me ajudar\n",
    "# a iteragir com o storage\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url = os.environ.get(\"MINIO_ENDPOINT\"),\n",
    "    aws_access_key_id = os.environ.get(\"MINIO_KEY\"),\n",
    "    aws_secret_access_key = os.environ.get(\"MINIO_SECRET\")\n",
    ")\n",
    "\n",
    "bucket_name = \"bank-databr\"\n",
    "\n",
    "# Paths Data Storage\n",
    "root_path_dir = f\"{bucket_name}\"\n",
    "landing_path_dir = f\"{root_path_dir}/landing/bacen\"\n",
    "bronze_path_dir = f\"{root_path_dir}/bronze\"\n",
    "silver_path_dir = f\"{root_path_dir}/silver\"\n",
    "\n",
    "# Partição da tabela Bronze, Data de referência\n",
    "dt_partition = datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12599abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "software.amazon.awssdk#s3 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-475f2364-f4c4-45ab-82b6-cdd48143126e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound software.amazon.awssdk#s3;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#aws-xml-protocol;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#aws-query-protocol;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#protocol-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#sdk-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#utils;2.26.30 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#endpoints-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-auth-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#identity-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-auth-aws;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#checksums-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#checksums;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#profiles;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#retries-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#retries;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#aws-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#regions;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#json-utils;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#third-party-jackson-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#auth;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-auth-aws-eventstream;2.26.30 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#http-auth;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#arns;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#crt-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#apache-client;2.26.30 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.17.1 in central\n",
      "\tfound software.amazon.awssdk#netty-nio-client;2.26.30 in central\n",
      "\tfound io.netty#netty-codec-http;4.1.111.Final in central\n",
      "\tfound io.netty#netty-common;4.1.111.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.111.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.111.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.111.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.111.Final in central\n",
      "\tfound io.netty#netty-handler;4.1.111.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.111.Final in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.111.Final in central\n",
      "\tfound io.netty#netty-transport-classes-epoll;4.1.111.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 2704ms :: artifacts dl 64ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.17.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-transport-classes-epoll;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.111.Final from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#apache-client;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#arns;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#auth;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-query-protocol;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-xml-protocol;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#checksums;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#checksums-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#crt-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#endpoints-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth-aws;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth-aws-eventstream;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#identity-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#json-utils;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#netty-nio-client;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#profiles;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#protocol-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#regions;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#retries;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#retries-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#s3;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#sdk-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#third-party-jackson-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   52  |   0   |   0   |   0   ||   52  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-475f2364-f4c4-45ab-82b6-cdd48143126e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 52 already retrieved (0kB/32ms)\n",
      "25/01/21 00:29:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"MeiosDePagamentoBancoCentral\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ[\"MINIO_ENDPOINT\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"MINIO_KEY\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"MINIO_SECRET\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582234d1-6097-42b7-8462-49f1295e900c",
   "metadata": {},
   "source": [
    "### 03. From Landing to Bronze\n",
    "\n",
    "Aqui vamos Ler os arquivos no formato original, extrair alguns metadados, e gravar no formato padrão, Delta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198a320",
   "metadata": {},
   "source": [
    "#### Funções Utilitárias - Storage Handler Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e152b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_table_exists_in_metastore(schema_name:str, table_name: str) -> bool:\n",
    "    \"\"\" Verifica se a tabela já existe no schema indicado dentro do Catalógo \"\"\"\n",
    "\n",
    "    return spark._jsparkSession.catalog() \\\n",
    "                               .tableExists(f\"{schema_name}.{table_name}\")\n",
    "\n",
    "\n",
    "def check_table_exists_by_location(s3_minio_client:boto3.client, destionation_table_path: str) -> bool:\n",
    "    \"\"\" Verifica se a tabela já existe, usando o gerenciador do storage \"\"\"\n",
    "\n",
    "    # Vou adaptar usando o boto3 já que não tenho por aqui nem o dbutils nem um metastore para checkar via tableExists()\n",
    "    \n",
    "    # Extrai o bucket e o prefixo (path)\n",
    "    bucket_name, path = destionation_table_path.replace(\"s3a://\", \"\").split(\"/\", 1)\n",
    "    \n",
    "    try:\n",
    "        # Verifica se o diretório existe no S3 (se o prefixo da pasta tiver objetos)\n",
    "        response = s3_minio_client.list_objects_v2(Bucket=bucket_name, Prefix=path, MaxKeys=1)\n",
    "        return 'Contents' in response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        return False\n",
    "\n",
    "\n",
    "def write_delta_table_by_location(df_input: DataFrame, destionation_table_path: str) -> None:\n",
    "    \"\"\" Escreve a tabela Delta no Storage de forma particionada\n",
    "        Se a tabela já existir realiza upsert na partição indicada\n",
    "        Caso não exista primeiramente cria a tabela no modo 'append'\n",
    "\n",
    "        Args:\n",
    "            df_input: dataframe a ser gravado no storage\n",
    "            destionation_table_path: local de destino no storage\n",
    "    \"\"\"\n",
    "\n",
    "    if check_table_exists_by_location(s3_client, destionation_table_path): \n",
    "\n",
    "        print(f\"Tabela já existe: escrevendo nova partição em:\\n\\t*{destionation_table_path}\")\n",
    "\n",
    "        df_input.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"replaceWhere\", f\"partition = {dt_partition}\") \\\n",
    "                .partitionBy(\"dt_partition\") \\\n",
    "                .save(destionation_table_path)   \n",
    "    else:\n",
    "        \n",
    "        print(f\"Tabela ainda não existe, criando nova tabela em: \\n\\t*{destionation_table_path}\")\n",
    "\n",
    "        #TODO: atualizar para considerar o EvolutionSchema, option(\"mergeSchema\": True)\n",
    "        # No formato atual estou considerando somente como EnforceSchema.\n",
    "\n",
    "        df_input.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .partitionBy(\"dt_partition\") \\\n",
    "                .save(destionation_table_path) \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa262e",
   "metadata": {},
   "source": [
    "#### 01. Meios de Pagamento Trimestral\n",
    "\n",
    "Todos os Meios de Pagamentos visão Trimestral (Landing to Bronze)\n",
    "Aqui vamos nos preocupar em:\n",
    "- Ler os dados no formato original, mantendo o schema da fonte\n",
    "- Capturar e adicionar alguns metadados sobre a versão, origem, formato, registros entre outros.\n",
    "- Gravar na Bronze em formato padronizado, usando Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8337dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. Leitura da fonte de Dados\n",
      "\t* Data Source Pagamentos Trimestrais: s3a://bank-databr/landing/bacen/meios_pagamentos_trimestral/data_18_01_2025_17_39_57.json\n",
      "\n",
      "* Schema Original do arquivo origem\n",
      "-RECORD 0------------------------------\n",
      " @odata.context | https://was-p.bcn... \n",
      " value          | [{2024-09-30, 152... \n",
      "\n",
      "02. Agora vamos capturar alguns metadados\n",
      "\t* Explorando a coluna _metadata\n",
      "-RECORD 0-------------------------\n",
      " _metadata | {s3a://bank-datab... \n",
      "\n",
      "-RECORD 0--------------------------------------\n",
      " @odata.context         | https://was-p.bcn... \n",
      " value                  | [{2024-09-30, 152... \n",
      " file_path              | s3a://bank-databr... \n",
      " file_name              | data_18_01_2025_1... \n",
      " file_size              | 19142                \n",
      " file_block_start       | 0                    \n",
      " file_block_length      | 19142                \n",
      " file_modification_time | 2025-01-18 20:40:39  \n",
      " ingestion_engine       | python_dlt           \n",
      " dt_partition           | 2025-01-21           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"01. Leitura da fonte de Dados\")\n",
    "\n",
    "data_source_file_path = \"s3a://\" + landing_path_dir + \"/meios_pagamentos_trimestral/data_18_01_2025_17_39_57.json\"\n",
    "print(f\"\\t* Data Source Pagamentos Trimestrais: {data_source_file_path}\")\n",
    "# Poderíamos usar também o data_*_.json para capturar todos os arquivos na origem.\n",
    "\n",
    "\n",
    "df_pagamentos_trimestral_raw = spark.read \\\n",
    "                                    .option(\"inferSchema\", True) \\\n",
    "                                    .json(data_source_file_path)\n",
    "\n",
    "print(\"\\n* Schema Original do arquivo origem\")\n",
    "df_pagamentos_trimestral_raw.printSchema\n",
    "df_pagamentos_trimestral_raw.show(n=1, vertical=True)\n",
    "\n",
    "\n",
    "print(\"02. Agora vamos capturar alguns metadados\")\n",
    "\n",
    "# Por padrão o spark já captura e armazena alguns metadadados default no Dataframe na coluna oculta _metadata\n",
    "print('\\t* Explorando a coluna _metadata')\n",
    "df_selected = df_pagamentos_trimestral_raw.select( \"_metadata\")\n",
    "df_selected.show(n=1, vertical=True)\n",
    "\n",
    "# Vou adicionar somente um campo a mais chamado: ingestion_engine: para marcar qual ferramenta ou fluxo foi o responsável pela ingestão\n",
    "# No nosso caso sempre um notebook-python com a lib DLT, mas poderia ser o recurso nify, airbyte, glue enfim, assim eu marcaria o id do recurso.\n",
    "\n",
    "df_pagamentos_trimestral_bronze = df_pagamentos_trimestral_raw.select(\n",
    "                                                                    col(\"*\"),\n",
    "                                                                    col('_metadata.*')                                                                \n",
    "                                                                ) \\\n",
    "                                                              .withColumn('ingestion_engine', lit('python_dlt')) \\\n",
    "                                                              .withColumn('dt_partition', lit(dt_partition))\n",
    "\n",
    "\n",
    "df_pagamentos_trimestral_bronze.show(n=1, vertical=True, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d730c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03. Gravando Dados na Bronze no Formato Delta\n",
      "Tabela já existe: escrevendo nova partição em:\n",
      "\t*s3a://bank-databr/bronze/b_pagamentos_trimestrais_bc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"03. Gravando Dados na Bronze no Formato Delta\")\n",
    "\n",
    "schema_name = \"db_bank_databr\"\n",
    "table_name = \"b_pagamentos_trimestrais_bc\"\n",
    "\n",
    "destionation_table_path = f\"s3a://{bronze_path_dir}/{table_name}\"\n",
    "\n",
    "write_delta_table_by_location(df_pagamentos_trimestral_bronze, destionation_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff45432-381d-4e1f-956b-2171a0fb2dcd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea65b6",
   "metadata": {},
   "source": [
    "#### 02. Meios de Pagamento Mensal\n",
    "\n",
    "Todos os Meios de Pagamentos visão Mensal (Landing to Bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe8f275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. Leitura da fonte de Dados\n",
      "\t* Data Source Pagamentos Mensais: s3a://bank-databr/landing/bacen/meios_pagamentos_mensal/data_18_01_2025_17_39_57.json\n",
      "\n",
      "* Schema Original do arquivo origem\n",
      "-RECORD 0------------------------------\n",
      " @odata.context | https://was-p.bcn... \n",
      " value          | [{202412, 361042.... \n",
      "\n",
      "02. Agora vamos capturar alguns metadados\n",
      "\t* Explorando a coluna _metadata\n",
      "-RECORD 0-------------------------\n",
      " _metadata | {s3a://bank-datab... \n",
      "\n",
      "-RECORD 0--------------------------------------\n",
      " @odata.context         | https://was-p.bcn... \n",
      " value                  | [{202412, 361042.... \n",
      " file_path              | s3a://bank-databr... \n",
      " file_name              | data_18_01_2025_1... \n",
      " file_size              | 3950                 \n",
      " file_block_start       | 0                    \n",
      " file_block_length      | 3950                 \n",
      " file_modification_time | 2025-01-18 20:40:20  \n",
      " ingestion_engine       | python_dlt           \n",
      " dt_partition           | 2025-01-21           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"01. Leitura da fonte de Dados\")\n",
    "\n",
    "data_source_file_path = \"s3a://\" + landing_path_dir + \"/meios_pagamentos_mensal/data_18_01_2025_17_39_57.json\"\n",
    "print(f\"\\t* Data Source Pagamentos Mensais: {data_source_file_path}\")\n",
    "# Poderíamos usar também o data_*_.json para capturar todos os arquivos na origem.\n",
    "\n",
    "\n",
    "df_pagamentos_mensal_raw = spark.read \\\n",
    "                                    .option(\"inferSchema\", True) \\\n",
    "                                    .json(data_source_file_path)\n",
    "\n",
    "print(\"\\n* Schema Original do arquivo origem\")\n",
    "df_pagamentos_mensal_raw.printSchema\n",
    "df_pagamentos_mensal_raw.show(n=1, vertical=True)\n",
    "\n",
    "\n",
    "print(\"02. Agora vamos capturar alguns metadados\")\n",
    "\n",
    "# Por padrão o spark já captura e armazena alguns metadadados default no Dataframe na coluna oculta _metadata\n",
    "print('\\t* Explorando a coluna _metadata')\n",
    "df_selected = df_pagamentos_mensal_raw.select( \"_metadata\")\n",
    "df_selected.show(n=1, vertical=True)\n",
    "\n",
    "# Vou adicionar somente um campo a mais chamado: ingestion_engine: para marcar qual ferramenta ou fluxo foi o responsável pela ingestão\n",
    "# No nosso caso sempre um notebook-python com a lib DLT, mas poderia ser o recurso nify, airbyte, glue enfim, assim eu marcaria o id do recurso.\n",
    "\n",
    "df_pagamentos_mensal_bronze = df_pagamentos_mensal_raw.select(\n",
    "                                                                  col(\"*\"),\n",
    "                                                                  col('_metadata.*')                                                                \n",
    "                                                           ) \\\n",
    "                                                      .withColumn('ingestion_engine', lit('python_dlt')) \\\n",
    "                                                      .withColumn('dt_partition', lit(dt_partition))\n",
    "\n",
    "\n",
    "df_pagamentos_mensal_bronze.show(n=1, vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208edc09-abe7-401f-b027-8de0f3b43041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03. Gravando Dados na Bronze no Formato Delta\n",
      "Tabela já existe: escrevendo nova partição em:\n",
      "\t*s3a://bank-databr/bronze/b_pagamentos_mensal_bc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"03. Gravando Dados na Bronze no Formato Delta\")\n",
    "\n",
    "schema_name = \"db_bank_databr\"\n",
    "table_name = \"b_pagamentos_mensal_bc\"\n",
    "\n",
    "destionation_table_path = f\"s3a://{bronze_path_dir}/{table_name}\"\n",
    "\n",
    "write_delta_table_by_location(df_pagamentos_mensal_bronze, destionation_table_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027dc2b1",
   "metadata": {},
   "source": [
    "#### 03. Meio de Pagamento - Cartões Trimestral\n",
    "\n",
    "Pagamentos com Cartão visão Trimestral (Landing to Bronze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770afe23-8118-4654-9162-ff438d5a053d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. Leitura da fonte de Dados\n",
      "\t* Data Source Pagamentos Mensais: s3a://bank-databr/landing/bacen/cartoes_trimestral/data_18_01_2025_17_39_57.json\n",
      "\n",
      "* Schema Original do arquivo origem\n",
      "-RECORD 0------------------------------\n",
      " @odata.context | https://was-p.bcn... \n",
      " value          | [{Diners Club, Cr... \n",
      "\n",
      "02. Agora vamos capturar alguns metadados\n",
      "\t* Explorando a coluna _metadata\n",
      "-RECORD 0-------------------------\n",
      " _metadata | {s3a://bank-datab... \n",
      "\n",
      "-RECORD 0--------------------------------------\n",
      " @odata.context         | https://was-p.bcn... \n",
      " value                  | [{Diners Club, Cr... \n",
      " file_path              | s3a://bank-databr... \n",
      " file_name              | data_18_01_2025_1... \n",
      " file_size              | 1074030              \n",
      " file_block_start       | 0                    \n",
      " file_block_length      | 1074030              \n",
      " file_modification_time | 2025-01-18 20:40:16  \n",
      " ingestion_engine       | python_dlt           \n",
      " dt_partition           | 2025-01-21           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"01. Leitura da fonte de Dados\")\n",
    "\n",
    "data_source_file_path = \"s3a://\" + landing_path_dir + \"/cartoes_trimestral/data_18_01_2025_17_39_57.json\"\n",
    "print(f\"\\t* Data Source Pagamentos Mensais: {data_source_file_path}\")\n",
    "# Poderíamos usar também o data_*_.json para capturar todos os arquivos na origem.\n",
    "\n",
    "\n",
    "df_cartoes_trimestral_raw = spark.read \\\n",
    "                                    .option(\"inferSchema\", True) \\\n",
    "                                    .json(data_source_file_path)\n",
    "\n",
    "print(\"\\n* Schema Original do arquivo origem\")\n",
    "df_cartoes_trimestral_raw.printSchema\n",
    "df_cartoes_trimestral_raw.show(n=1, vertical=True)\n",
    "\n",
    "\n",
    "print(\"02. Agora vamos capturar alguns metadados\")\n",
    "\n",
    "# Por padrão o spark já captura e armazena alguns metadadados default no Dataframe na coluna oculta _metadata\n",
    "print('\\t* Explorando a coluna _metadata')\n",
    "df_selected = df_cartoes_trimestral_raw.select( \"_metadata\")\n",
    "df_selected.show(n=1, vertical=True)\n",
    "\n",
    "# Vou adicionar somente um campo a mais chamado: ingestion_engine: para marcar qual ferramenta ou fluxo foi o responsável pela ingestão\n",
    "# No nosso caso sempre um notebook-python com a lib DLT, mas poderia ser o recurso nify, airbyte, glue enfim, assim eu marcaria o id do recurso.\n",
    "\n",
    "df_cartoes_trimestral_bronze = df_cartoes_trimestral_raw.select(\n",
    "                                                                  col(\"*\"),\n",
    "                                                                  col('_metadata.*')                                                                \n",
    "                                                           ) \\\n",
    "                                                      .withColumn('ingestion_engine', lit('python_dlt')) \\\n",
    "                                                      .withColumn('dt_partition', lit(dt_partition))\n",
    "\n",
    "\n",
    "df_cartoes_trimestral_bronze.show(n=1, vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35fe4110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03. Gravando Dados na Bronze no Formato Delta\n",
      "Tabela já existe: escrevendo nova partição em:\n",
      "\t*s3a://bank-databr/bronze/b_cartoes_trimestral_bc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"03. Gravando Dados na Bronze no Formato Delta\")\n",
    "\n",
    "schema_name = \"db_bank_databr\"\n",
    "table_name = \"b_cartoes_trimestral_bc\"\n",
    "\n",
    "destionation_table_path = f\"s3a://{bronze_path_dir}/{table_name}\"\n",
    "\n",
    "write_delta_table_by_location(df_cartoes_trimestral_bronze, destionation_table_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
