{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulação e Tratamento de Dados - Camada Silver\n",
    "\n",
    "Camada de dados tratada e normalizada, confíavel para uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTS AND LIBRARIES\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configuração do logger\n",
    "logger = logging.getLogger(\"minio_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Configurando o formato do log\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "# PySpark Libraries\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, explode, lit, from_json\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis Globais e de Ambiente para o Projeto.\n",
    "\n",
    "os.environ[\"MINIO_KEY\"] = \"developer\"\n",
    "os.environ[\"MINIO_SECRET\"] = \"developer01\"\n",
    "os.environ[\"MINIO_ENDPOINT\"] = \"http://minio:9000\"\n",
    "\n",
    "\n",
    "# Na ausência do dbtuils ou do Metastore vou usar o boto para me ajudar\n",
    "# a iteragir com o storage\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url = os.environ.get(\"MINIO_ENDPOINT\"),\n",
    "    aws_access_key_id = os.environ.get(\"MINIO_KEY\"),\n",
    "    aws_secret_access_key = os.environ.get(\"MINIO_SECRET\")\n",
    ")\n",
    "\n",
    "bucket_name = \"bank-databr\"\n",
    "\n",
    "# Paths Data Storage\n",
    "root_path_dir = f\"{bucket_name}\"\n",
    "landing_path_dir = f\"{root_path_dir}/landing/bacen\"\n",
    "bronze_path_dir = f\"{root_path_dir}/bronze\"\n",
    "silver_path_dir = f\"{root_path_dir}/silver\"\n",
    "\n",
    "# Partição da tabela Bronze, Data de referência\n",
    "dt_partition = datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"SilverLayer\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ[\"MINIO_ENDPOINT\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"MINIO_KEY\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"MINIO_SECRET\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções Utilitárias - Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_table_exists_in_metastore(schema_name:str, table_name: str) -> bool:\n",
    "    \"\"\" Verifica se a tabela já existe no schema indicado dentro do Catalógo \"\"\"\n",
    "\n",
    "    return spark._jsparkSession.catalog() \\\n",
    "                               .tableExists(f\"{schema_name}.{table_name}\")\n",
    "\n",
    "\n",
    "def check_table_exists_by_location(s3_minio_client:boto3.client, destionation_table_path: str) -> bool:\n",
    "    \"\"\" Verifica se a tabela já existe, usando o gerenciador do storage \"\"\"\n",
    "\n",
    "    # Vou adaptar usando o boto3 já que não tenho por aqui nem o dbutils nem um metastore para checkar via tableExists()\n",
    "    \n",
    "    # Extrai o bucket e o prefixo (path)\n",
    "    bucket_name, path = destionation_table_path.replace(\"s3a://\", \"\").split(\"/\", 1)\n",
    "    \n",
    "    try:\n",
    "        # Verifica se o diretório existe no S3 (se o prefixo da pasta tiver objetos)\n",
    "        response = s3_minio_client.list_objects_v2(Bucket=bucket_name, Prefix=path, MaxKeys=1)\n",
    "        return 'Contents' in response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        return False\n",
    "\n",
    "def write_delta_table_by_location(df_input: DataFrame, destionation_table_path: str) -> None:\n",
    "    \"\"\" Escreve a tabela Delta no Storage de forma particionada\n",
    "        Se a tabela já existir realiza upsert na partição indicada\n",
    "        Caso não exista primeiramente cria a tabela no modo 'append'\n",
    "\n",
    "        Args:\n",
    "            df_input: dataframe a ser gravado no storage\n",
    "            destionation_table_path: local de destino no storage\n",
    "    \"\"\"\n",
    "\n",
    "    if check_table_exists_by_location(s3_client, destionation_table_path): \n",
    "\n",
    "        print(f\"Tabela já existe: escrevendo nova partição em:\\n\\t*{destionation_table_path}\")\n",
    "\n",
    "        df_input.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"replaceWhere\", f\"partition = {dt_partition}\") \\\n",
    "                .partitionBy(\"dt_partition\") \\\n",
    "                .save(destionation_table_path)   \n",
    "    else:\n",
    "        \n",
    "        print(f\"Tabela ainda não existe, criando nova tabela em: \\n\\t*{destionation_table_path}\")\n",
    "\n",
    "        #TODO: atualizar para considerar o EvolutionSchema, option(\"mergeSchema\": True)\n",
    "        # No formato atual estou considerando somente como EnforceSchema.\n",
    "\n",
    "        df_input.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .partitionBy(\"dt_partition\") \\\n",
    "                .save(destionation_table_path) \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Pagamentos  Trimestrais\n",
    "\n",
    "Aqui vamos ler a tabela \"as-is\" da bronze e aplicar algumas transformações, tratativas na base de dados final para gravação na Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Data Source Pagamentos Trimestrais: s3a://bank-databr/bronze/b_pagamentos_trimestrais_bc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/26 01:16:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Schema Original do arquivo origem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " @odata.context         | https://was-p.bcn... \n",
      " value                  | [{2024-09-30, 152... \n",
      " file_path              | s3a://bank-databr... \n",
      " file_name              | data_18_01_2025_1... \n",
      " file_size              | 19142                \n",
      " file_block_start       | 0                    \n",
      " file_block_length      | 19142                \n",
      " file_modification_time | 2025-01-18 20:40:39  \n",
      " ingestion_engine       | python_dlt           \n",
      " dt_partition           | 2025-01-20           \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lendo tabela de Pagamentos Trimestrais da Bronze\n",
    "\n",
    "table_name = \"b_pagamentos_trimestrais_bc\"\n",
    "dt_ref_carga= \"2025-01-20\"\n",
    "# Fixando a partição a ser lida, se fosse algo produtivo, e via orquestrador como Control-M ou Airflow\n",
    "# seria uma variável de ambiente com este valor\n",
    "\n",
    "data_source_file_path = f\"s3a://{bronze_path_dir}/{table_name}\"\n",
    "print(f\"\\t* Data Source Pagamentos Trimestrais: {data_source_file_path}\")\n",
    "# Poderíamos usar também o data_*_.json para capturar todos os arquivos na origem.\n",
    "\n",
    "\n",
    "df_pagamentos_trimestral_bronze = spark.read \\\n",
    "                                       .format(\"delta\") \\\n",
    "                                       .load(data_source_file_path) \\\n",
    "                                       .where(col('dt_partition') == dt_ref_carga)\n",
    "\n",
    "print(\"\\n* Schema Original do arquivo origem\")\n",
    "df_pagamentos_trimestral_bronze.printSchema\n",
    "df_pagamentos_trimestral_bronze.show(n=1, vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datatrimestre: string (nullable = true)\n",
      " |-- quantidadeBoleto: double (nullable = true)\n",
      " |-- quantidadeCartaoCredito: double (nullable = true)\n",
      " |-- quantidadeCartaoDebito: double (nullable = true)\n",
      " |-- quantidadeCartaoPrePago: double (nullable = true)\n",
      " |-- quantidadeCheque: double (nullable = true)\n",
      " |-- quantidadeConvenios: double (nullable = true)\n",
      " |-- quantidadeDOC: double (nullable = true)\n",
      " |-- quantidadeDebitoDireto: double (nullable = true)\n",
      " |-- quantidadePix: double (nullable = true)\n",
      " |-- quantidadeSaques: double (nullable = true)\n",
      " |-- quantidadeTEC: double (nullable = true)\n",
      " |-- quantidadeTED: double (nullable = true)\n",
      " |-- quantidadeTransIntrabancaria: double (nullable = true)\n",
      " |-- valorBoleto: double (nullable = true)\n",
      " |-- valorCartaoCredito: double (nullable = true)\n",
      " |-- valorCartaoDebito: double (nullable = true)\n",
      " |-- valorCartaoPrePago: double (nullable = true)\n",
      " |-- valorCheque: double (nullable = true)\n",
      " |-- valorConvenios: double (nullable = true)\n",
      " |-- valorDOC: double (nullable = true)\n",
      " |-- valorDebitoDireto: double (nullable = true)\n",
      " |-- valorPix: double (nullable = true)\n",
      " |-- valorSaques: double (nullable = true)\n",
      " |-- valorTEC: double (nullable = true)\n",
      " |-- valorTED: double (nullable = true)\n",
      " |-- valorTransIntrabancaria: double (nullable = true)\n",
      " |-- dt_partition: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------------------+----------------------+-----------------------+----------------+-------------------+-------------+----------------------+-------------+----------------+-------------+-------------+----------------------------+-----------+------------------+-----------------+------------------+-----------+--------------+--------+-----------------+----------+-----------+--------+-------------+-----------------------+------------+\n",
      "|datatrimestre|quantidadeBoleto|quantidadeCartaoCredito|quantidadeCartaoDebito|quantidadeCartaoPrePago|quantidadeCheque|quantidadeConvenios|quantidadeDOC|quantidadeDebitoDireto|quantidadePix|quantidadeSaques|quantidadeTEC|quantidadeTED|quantidadeTransIntrabancaria|valorBoleto|valorCartaoCredito|valorCartaoDebito|valorCartaoPrePago|valorCheque|valorConvenios|valorDOC|valorDebitoDireto|  valorPix|valorSaques|valorTEC|     valorTED|valorTransIntrabancaria|dt_partition|\n",
      "+-------------+----------------+-----------------------+----------------------+-----------------------+----------------+-------------------+-------------+----------------------+-------------+----------------+-------------+-------------+----------------------------+-----------+------------------+-----------------+------------------+-----------+--------------+--------+-----------------+----------+-----------+--------+-------------+-----------------------+------------+\n",
      "|   2024-09-30|       1525449.9|             5061154.29|             4121806.1|             3132432.21|        44243.73|          661708.42|          0.0|            4056152.31|1.654642727E7|       664224.81|          0.0|    203394.26|                   771817.52| 2463425.06|         662702.46|        241932.98|          78531.64|   194330.6|     961282.82|     0.0|        676418.19|6975429.47|  526775.87|     0.0|1.118021768E7|             6746301.96|  2025-01-20|\n",
      "|   2024-06-30|      1439119.22|             4879670.45|            4057958.85|             3046202.02|        44981.97|          679409.28|          0.0|            3320559.12|1.535689092E7|       698602.29|          0.0|    203269.58|                   357401.33| 2360557.98|         631806.82|        239101.56|          77386.79|  221553.82|     984002.13|     0.0|        589043.45|6284296.05|  591903.75|     0.0|1.066255573E7|             6190513.44|  2025-01-20|\n",
      "|   2024-03-31|      1422709.26|             4723437.71|            3952365.16|             2774237.04|        46468.88|          726660.81|       662.65|            3238627.21|1.357545559E7|        683041.3|       447.19|    203714.16|                   321909.52| 2248815.45|         607470.96|         235111.6|          73035.61|  190457.25|    1082621.22|   650.8|        568515.99|5429305.43|  522123.87|  925.26|   9651625.56|             5396558.52|  2025-01-20|\n",
      "+-------------+----------------+-----------------------+----------------------+-----------------------+----------------+-------------------+-------------+----------------------+-------------+----------------+-------------+-------------+----------------------------+-----------+------------------+-----------------+------------------+-----------+--------------+--------+-----------------+----------+-----------+--------+-------------+-----------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos transformar a tabela original, explodindo os campos JSON/Struct para novas colunas e filtrando algumas variáveis\n",
    "\n",
    "# Explodindo o valor da coluna \"value\" em múltiplas colunas\n",
    "df_pagamentos_trimestral_transformed = df_pagamentos_trimestral_bronze.withColumn('value_struct', explode(col(\"value\"))) \\\n",
    "                                                                      .select( col(\"value_struct.*\") ) \\\n",
    "                                                                      .drop(*['@odata.context', 'value']) \\\n",
    "                                                                      .withColumn('dt_partition', lit(dt_ref_carga) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Schema Original do arquivo origem\n",
    "df_pagamentos_trimestral_transformed.printSchema()\n",
    "df_pagamentos_trimestral_transformed.show(n=3, vertical=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality com Great Expectations\n",
    "\n",
    "Vamos usar o Great Expectations para de forma padronizada, validar os intervaloes e valores das coluns em nosso Dataframe\n",
    "\n",
    "1. Check for duplicates\n",
    "2. Check for unique values in columns\n",
    "3. Check for missing values\n",
    "4. Categorical value distributions\n",
    "5. Schema validation\n",
    "6. Temporal consistency check\n",
    "7. Cross-field validation\n",
    "8. Dependency check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_get():\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    context = great_expectations.get_context()\n",
    "\n",
    "    context.add_expectation_suite(\n",
    "        expectation_suite = suite_name\n",
    "    )\n",
    "\n",
    "    context.add_datasource(**yaml.load(datasource_yaml))\n",
    "    config_data_docs_site(context, output_path)\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tests_suite(df_pagamentos_trimestrais: DataFrame):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    column_list = []\n",
    "\n",
    "    df_pagamentos_trimestrais.expected_table_columns_to_be_unique(\"\")\n",
    "    df_pagamentos_trimestrais.expect_column_values_to_not_be_null(\"product_id\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
