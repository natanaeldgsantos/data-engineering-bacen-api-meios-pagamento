{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulação e Tratamento de Dados - Camada Silver\n",
    "\n",
    "Camada de dados tratada e normalizada, confíavel para uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTS AND LIBRARIES\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configuração do logger\n",
    "logger = logging.getLogger(\"minio_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Configurando o formato do log\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "# PySpark Libraries\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, explode, lit, from_json\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis Globais e de Ambiente para o Projeto.\n",
    "\n",
    "os.environ[\"MINIO_KEY\"] = \"developer\"\n",
    "os.environ[\"MINIO_SECRET\"] = \"developer01\"\n",
    "os.environ[\"MINIO_ENDPOINT\"] = \"http://minio:9000\"\n",
    "\n",
    "\n",
    "# Na ausência do dbtuils ou do Metastore vou usar o boto para me ajudar\n",
    "# a iteragir com o storage\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url = os.environ.get(\"MINIO_ENDPOINT\"),\n",
    "    aws_access_key_id = os.environ.get(\"MINIO_KEY\"),\n",
    "    aws_secret_access_key = os.environ.get(\"MINIO_SECRET\")\n",
    ")\n",
    "\n",
    "bucket_name = \"bank-databr\"\n",
    "\n",
    "# Paths Data Storage\n",
    "root_path_dir = f\"{bucket_name}\"\n",
    "landing_path_dir = f\"{root_path_dir}/landing/bacen\"\n",
    "bronze_path_dir = f\"{root_path_dir}/bronze\"\n",
    "silver_path_dir = f\"{root_path_dir}/silver\"\n",
    "\n",
    "# Partição da tabela Bronze, Data de referência\n",
    "dt_partition = datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "software.amazon.awssdk#s3 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c3be89e5-3da9-494e-8616-cb8cc16109e3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound software.amazon.awssdk#s3;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#aws-xml-protocol;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#aws-query-protocol;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#protocol-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#sdk-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#utils;2.26.30 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#endpoints-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-auth-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#identity-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-auth-aws;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#checksums-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#checksums;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#profiles;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#retries-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#retries;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#aws-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#regions;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#json-utils;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#third-party-jackson-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#auth;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-auth-aws-eventstream;2.26.30 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#http-auth;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#arns;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#crt-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#apache-client;2.26.30 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.17.1 in central\n",
      "\tfound software.amazon.awssdk#netty-nio-client;2.26.30 in central\n",
      "\tfound io.netty#netty-codec-http;4.1.111.Final in central\n",
      "\tfound io.netty#netty-common;4.1.111.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.111.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.111.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.111.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.111.Final in central\n",
      "\tfound io.netty#netty-handler;4.1.111.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.111.Final in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.111.Final in central\n",
      "\tfound io.netty#netty-transport-classes-epoll;4.1.111.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 4131ms :: artifacts dl 69ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.17.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-transport-classes-epoll;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.111.Final from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#apache-client;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#arns;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#auth;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-query-protocol;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-xml-protocol;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#checksums;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#checksums-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#crt-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#endpoints-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth-aws;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth-aws-eventstream;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#identity-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#json-utils;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#netty-nio-client;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#profiles;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#protocol-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#regions;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#retries;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#retries-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#s3;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#sdk-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#third-party-jackson-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   52  |   0   |   0   |   0   ||   52  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c3be89e5-3da9-494e-8616-cb8cc16109e3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 52 already retrieved (0kB/75ms)\n",
      "25/01/30 23:18:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"SilverLayer\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ[\"MINIO_ENDPOINT\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"MINIO_KEY\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"MINIO_SECRET\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções Utilitárias - Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_table_exists_in_metastore(schema_name:str, table_name: str) -> bool:\n",
    "    \"\"\" Verifica se a tabela já existe no schema indicado dentro do Catalógo \"\"\"\n",
    "\n",
    "    return spark._jsparkSession.catalog() \\\n",
    "                               .tableExists(f\"{schema_name}.{table_name}\")\n",
    "\n",
    "\n",
    "def check_table_exists_by_location(s3_minio_client:boto3.client, destionation_table_path: str) -> bool:\n",
    "    \"\"\" Verifica se a tabela já existe, usando o gerenciador do storage \"\"\"\n",
    "\n",
    "    # Vou adaptar usando o boto3 já que não tenho por aqui nem o dbutils nem um metastore para checkar via tableExists()\n",
    "    \n",
    "    # Extrai o bucket e o prefixo (path)\n",
    "    bucket_name, path = destionation_table_path.replace(\"s3a://\", \"\").split(\"/\", 1)\n",
    "    \n",
    "    try:\n",
    "        # Verifica se o diretório existe no S3 (se o prefixo da pasta tiver objetos)\n",
    "        response = s3_minio_client.list_objects_v2(Bucket=bucket_name, Prefix=path, MaxKeys=1)\n",
    "        return 'Contents' in response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        return False\n",
    "\n",
    "def write_delta_table_by_location(df_input: DataFrame, destionation_table_path: str) -> None:\n",
    "    \"\"\" Escreve a tabela Delta no Storage de forma particionada\n",
    "        Se a tabela já existir realiza upsert na partição indicada\n",
    "        Caso não exista primeiramente cria a tabela no modo 'append'\n",
    "\n",
    "        Args:\n",
    "            df_input: dataframe a ser gravado no storage\n",
    "            destionation_table_path: local de destino no storage\n",
    "    \"\"\"\n",
    "\n",
    "    if check_table_exists_by_location(s3_client, destionation_table_path): \n",
    "\n",
    "        print(f\"Tabela já existe: escrevendo nova partição em:\\n\\t*{destionation_table_path}\")\n",
    "\n",
    "        df_input.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"replaceWhere\", f\"partition = {dt_partition}\") \\\n",
    "                .partitionBy(\"dt_partition\") \\\n",
    "                .save(destionation_table_path)   \n",
    "    else:\n",
    "        \n",
    "        print(f\"Tabela ainda não existe, criando nova tabela em: \\n\\t*{destionation_table_path}\")\n",
    "\n",
    "        #TODO: atualizar para considerar o EvolutionSchema, option(\"mergeSchema\": True)\n",
    "        # No formato atual estou considerando somente como EnforceSchema.\n",
    "\n",
    "        df_input.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .partitionBy(\"dt_partition\") \\\n",
    "                .save(destionation_table_path) \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Pagamentos  Trimestrais\n",
    "\n",
    "\n",
    "Leitura e tratamento da tabela Bronze. Aqui vamos aplicar algumas transformações e tratamentos também vamos definir um schema final desejado para a tabela na silva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Data Source Pagamentos Trimestrais: s3a://bank-databr/bronze/b_pagamentos_trimestrais_bc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/30 23:20:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Schema Original do arquivo origem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " @odata.context         | https://was-p.bcn... \n",
      " value                  | [{2024-09-30, 152... \n",
      " file_path              | s3a://bank-databr... \n",
      " file_name              | data_18_01_2025_1... \n",
      " file_size              | 19142                \n",
      " file_block_start       | 0                    \n",
      " file_block_length      | 19142                \n",
      " file_modification_time | 2025-01-18 20:40:39  \n",
      " ingestion_engine       | python_dlt           \n",
      " dt_partition           | 2025-01-20           \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lendo tabela de Pagamentos Trimestrais da Bronze\n",
    "\n",
    "table_name = \"b_pagamentos_trimestrais_bc\"\n",
    "dt_ref_carga= \"2025-01-20\"\n",
    "# Fixando a partição a ser lida, se fosse algo produtivo, e via orquestrador como Control-M ou Airflow\n",
    "# seria uma variável de ambiente com este valor\n",
    "\n",
    "data_source_file_path = f\"s3a://{bronze_path_dir}/{table_name}\"\n",
    "print(f\"* Data Source Pagamentos Trimestrais: {data_source_file_path}\")\n",
    "\n",
    "\n",
    "df_pagamentos_trimestral_bronze = spark.read \\\n",
    "                                       .format(\"delta\") \\\n",
    "                                       .load(data_source_file_path) \\\n",
    "                                       .where(col('dt_partition') == dt_ref_carga)\n",
    "\n",
    "\n",
    "print(\"\\n* Schema Original do arquivo origem\")\n",
    "df_pagamentos_trimestral_bronze.printSchema\n",
    "df_pagamentos_trimestral_bronze.show(n=1, vertical=True, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicando Transformações e Schema Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datatrimestre: string (nullable = true)\n",
      " |-- quantidadeBoleto: double (nullable = true)\n",
      " |-- quantidadeCartaoCredito: double (nullable = true)\n",
      " |-- quantidadeCartaoDebito: double (nullable = true)\n",
      " |-- quantidadeCartaoPrePago: double (nullable = true)\n",
      " |-- quantidadeCheque: double (nullable = true)\n",
      " |-- quantidadeConvenios: double (nullable = true)\n",
      " |-- quantidadeDOC: double (nullable = true)\n",
      " |-- quantidadeDebitoDireto: double (nullable = true)\n",
      " |-- quantidadePix: double (nullable = true)\n",
      " |-- quantidadeSaques: double (nullable = true)\n",
      " |-- quantidadeTEC: double (nullable = true)\n",
      " |-- quantidadeTED: double (nullable = true)\n",
      " |-- quantidadeTransIntrabancaria: double (nullable = true)\n",
      " |-- valorBoleto: double (nullable = true)\n",
      " |-- valorCartaoCredito: double (nullable = true)\n",
      " |-- valorCartaoDebito: double (nullable = true)\n",
      " |-- valorCartaoPrePago: double (nullable = true)\n",
      " |-- valorCheque: double (nullable = true)\n",
      " |-- valorConvenios: double (nullable = true)\n",
      " |-- valorDOC: double (nullable = true)\n",
      " |-- valorDebitoDireto: double (nullable = true)\n",
      " |-- valorPix: double (nullable = true)\n",
      " |-- valorSaques: double (nullable = true)\n",
      " |-- valorTEC: double (nullable = true)\n",
      " |-- valorTED: double (nullable = true)\n",
      " |-- valorTransIntrabancaria: double (nullable = true)\n",
      " |-- dt_partition: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------------------+----------------------+-----------------------+----------------+-------------------+-------------+----------------------+-------------+----------------+-------------+-------------+----------------------------+-----------+------------------+-----------------+------------------+-----------+--------------+--------+-----------------+----------+-----------+--------+-------------+-----------------------+------------+\n",
      "|datatrimestre|quantidadeBoleto|quantidadeCartaoCredito|quantidadeCartaoDebito|quantidadeCartaoPrePago|quantidadeCheque|quantidadeConvenios|quantidadeDOC|quantidadeDebitoDireto|quantidadePix|quantidadeSaques|quantidadeTEC|quantidadeTED|quantidadeTransIntrabancaria|valorBoleto|valorCartaoCredito|valorCartaoDebito|valorCartaoPrePago|valorCheque|valorConvenios|valorDOC|valorDebitoDireto|  valorPix|valorSaques|valorTEC|     valorTED|valorTransIntrabancaria|dt_partition|\n",
      "+-------------+----------------+-----------------------+----------------------+-----------------------+----------------+-------------------+-------------+----------------------+-------------+----------------+-------------+-------------+----------------------------+-----------+------------------+-----------------+------------------+-----------+--------------+--------+-----------------+----------+-----------+--------+-------------+-----------------------+------------+\n",
      "|   2024-09-30|       1525449.9|             5061154.29|             4121806.1|             3132432.21|        44243.73|          661708.42|          0.0|            4056152.31|1.654642727E7|       664224.81|          0.0|    203394.26|                   771817.52| 2463425.06|         662702.46|        241932.98|          78531.64|   194330.6|     961282.82|     0.0|        676418.19|6975429.47|  526775.87|     0.0|1.118021768E7|             6746301.96|  2025-01-20|\n",
      "|   2024-06-30|      1439119.22|             4879670.45|            4057958.85|             3046202.02|        44981.97|          679409.28|          0.0|            3320559.12|1.535689092E7|       698602.29|          0.0|    203269.58|                   357401.33| 2360557.98|         631806.82|        239101.56|          77386.79|  221553.82|     984002.13|     0.0|        589043.45|6284296.05|  591903.75|     0.0|1.066255573E7|             6190513.44|  2025-01-20|\n",
      "|   2024-03-31|      1422709.26|             4723437.71|            3952365.16|             2774237.04|        46468.88|          726660.81|       662.65|            3238627.21|1.357545559E7|        683041.3|       447.19|    203714.16|                   321909.52| 2248815.45|         607470.96|         235111.6|          73035.61|  190457.25|    1082621.22|   650.8|        568515.99|5429305.43|  522123.87|  925.26|   9651625.56|             5396558.52|  2025-01-20|\n",
      "+-------------+----------------+-----------------------+----------------------+-----------------------+----------------+-------------------+-------------+----------------------+-------------+----------------+-------------+-------------+----------------------------+-----------+------------------+-----------------+------------------+-----------+--------------+--------+-----------------+----------+-----------+--------+-------------+-----------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos transformar a tabela original, explodindo os campos JSON/Struct para novas colunas e filtrando algumas variáveis\n",
    "\n",
    "# Explodindo o valor da coluna \"value\" em múltiplas colunas\n",
    "df_pagamentos_trimestral_transformed = df_pagamentos_trimestral_bronze.withColumn('value_struct', explode(col(\"value\"))) \\\n",
    "                                                                      .select( col(\"value_struct.*\") ) \\\n",
    "                                                                      .drop(*['@odata.context', 'value']) \\\n",
    "                                                                      .withColumn('dt_partition', lit(dt_ref_carga) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Schema Original do arquivo origem\n",
    "df_pagamentos_trimestral_transformed.printSchema()\n",
    "df_pagamentos_trimestral_transformed.show(n=3, vertical=False, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality\n",
    "\n",
    "Vamos verificar o schema inferido validando os tipos dos dados e valores carregados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------+-------------+\n",
      "|ColumnName                  |DataType|CountMissings|\n",
      "+----------------------------+--------+-------------+\n",
      "|datatrimestre               |string  |0            |\n",
      "|quantidadeBoleto            |double  |0            |\n",
      "|quantidadeCartaoCredito     |double  |0            |\n",
      "|quantidadeCartaoDebito      |double  |0            |\n",
      "|quantidadeCartaoPrePago     |double  |0            |\n",
      "|quantidadeCheque            |double  |0            |\n",
      "|quantidadeConvenios         |double  |0            |\n",
      "|quantidadeDOC               |double  |0            |\n",
      "|quantidadeDebitoDireto      |double  |0            |\n",
      "|quantidadePix               |double  |0            |\n",
      "|quantidadeSaques            |double  |0            |\n",
      "|quantidadeTEC               |double  |0            |\n",
      "|quantidadeTED               |double  |0            |\n",
      "|quantidadeTransIntrabancaria|double  |0            |\n",
      "|valorBoleto                 |double  |0            |\n",
      "|valorCartaoCredito          |double  |0            |\n",
      "|valorCartaoDebito           |double  |0            |\n",
      "|valorCartaoPrePago          |double  |0            |\n",
      "|valorCheque                 |double  |0            |\n",
      "|valorConvenios              |double  |0            |\n",
      "+----------------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import count as _count, when\n",
    "\n",
    "def resumo_estatistico(df: DataFrame) -> DataFrame:\n",
    "    \"\"\" Gera um resumo com metadados e estatíticas básicas sobre o DataFrame \"\"\"\n",
    "\n",
    "    # Recuperando o Nome e tipos de cada Coluna\n",
    "    column_types = df.dtypes\n",
    "\n",
    "    # Recupera a quantidade de valores Nulos/Empty/Missing para cada coluna\n",
    "\n",
    "    null_counts = df.select([\n",
    "                                _count(\n",
    "                                    when(col(column_name).isNull(), column_name)\n",
    "                                ).alias(column_name)\n",
    "                                for column_name in df.columns\n",
    "                            ]).collect()[0]\n",
    "    \n",
    "    # Montando o resumo final\n",
    "    resumo = [\n",
    "        (column_name, dtype, null_counts[column_name])\n",
    "        for column_name, dtype in column_types\n",
    "    ]\n",
    "\n",
    "    # Dataframe resposta\n",
    "    schema = [\"ColumnName\", \"DataType\", \"CountMissings\"]\n",
    "\n",
    "    return spark.createDataFrame(data = resumo, schema = schema)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_statistics = resumo_estatistico(df_pagamentos_trimestral_transformed)\n",
    "\n",
    "df_statistics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality com Great Expectations\n",
    "\n",
    "Vamos usar o Great Expectations para de forma padronizada, validar os intervaloes e valores das coluns em nosso Dataframe\n",
    "\n",
    "1. Check for duplicates\n",
    "2. Check for unique values in columns\n",
    "3. Check for missing values\n",
    "4. Categorical value distributions\n",
    "5. Schema validation\n",
    "6. Temporal consistency check\n",
    "7. Cross-field validation\n",
    "8. Dependency check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import great_expectations as gx\n",
    "import great_expectations.expectations as gxe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Criando o contexto\n",
    "context = gx.get_context()\n",
    "\n",
    "\n",
    "# Configurando fonte de dados\n",
    "datasource = context.data_sources.add_spark(name = \"pagamentos_trimestrais\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando asset\n",
    "data_asset = datasource.add_dataframe_asset(name = \"pagamentos_trimestrais_asset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando Definições Batch\n",
    "\n",
    "batch_definition = data_asset.add_batch_definition_whole_dataframe(\"batch_pagamentos_trimestrais\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando dataframe como parâmetro\n",
    "\n",
    "batch_parameters = {\"dataframe\": df_pagamentos_trimestral_transformed}\n",
    "\n",
    "batch = batch_definition.get_batch(batch_parameters= batch_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 13/13 [00:02<00:00,  4.85it/s]            \n"
     ]
    }
   ],
   "source": [
    "expectation = gx.expectations.ExpectColumnValuesToBeBetween(\n",
    "    column = \"quantidadeBoleto\",\n",
    "    min_value = 1,\n",
    "    max_value = 100\n",
    ")\n",
    "\n",
    "\n",
    "validation_result = batch.validate(expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"success\": false,\n",
      "  \"expectation_config\": {\n",
      "    \"type\": \"expect_column_values_to_be_between\",\n",
      "    \"kwargs\": {\n",
      "      \"batch_id\": \"pagamentos_trimestrais-pagamentos_trimestrais_asset\",\n",
      "      \"column\": \"quantidadeBoleto\",\n",
      "      \"min_value\": 1.0,\n",
      "      \"max_value\": 100.0\n",
      "    },\n",
      "    \"meta\": {}\n",
      "  },\n",
      "  \"result\": {\n",
      "    \"element_count\": 161,\n",
      "    \"unexpected_count\": 161,\n",
      "    \"unexpected_percent\": 100.0,\n",
      "    \"partial_unexpected_list\": [\n",
      "      1525449.9,\n",
      "      1439119.22,\n",
      "      1422709.26,\n",
      "      1452479.44,\n",
      "      1487093.97,\n",
      "      1494217.09,\n",
      "      1510864.63,\n",
      "      1522483.73,\n",
      "      1538367.11,\n",
      "      1517092.79,\n",
      "      1490934.41,\n",
      "      1497704.81,\n",
      "      1493638.16,\n",
      "      1431427.43,\n",
      "      1386080.6,\n",
      "      1472868.82,\n",
      "      1389215.55,\n",
      "      1256812.05,\n",
      "      1184510.67,\n",
      "      1190397.8\n",
      "    ],\n",
      "    \"missing_count\": 0,\n",
      "    \"missing_percent\": 0.0,\n",
      "    \"unexpected_percent_total\": 100.0,\n",
      "    \"unexpected_percent_nonmissing\": 100.0,\n",
      "    \"partial_unexpected_counts\": [\n",
      "      {\n",
      "        \"value\": 1184510.67,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1190397.8,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1256812.05,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1386080.6,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1389215.55,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1422709.26,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1431427.43,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1439119.22,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1452479.44,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1472868.82,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1487093.97,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1490934.41,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1493638.16,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1494217.09,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1497704.81,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1510864.63,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1517092.79,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1522483.73,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1525449.9,\n",
      "        \"count\": 1\n",
      "      },\n",
      "      {\n",
      "        \"value\": 1538367.11,\n",
      "        \"count\": 1\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"meta\": {},\n",
      "  \"exception_info\": {\n",
      "    \"raised_exception\": false,\n",
      "    \"exception_traceback\": null,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(validation_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
