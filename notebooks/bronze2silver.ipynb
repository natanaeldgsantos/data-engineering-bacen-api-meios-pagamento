{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulação e Tratamento de Dados - Camada Silver\n",
    "\n",
    "Camada de dados tratada e normalizada, confíavel para uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTS AND LIBRARIES\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import great_expectations\n",
    "\n",
    "# Configuração do logger\n",
    "logger = logging.getLogger(\"minio_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Configurando o formato do log\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "# PySpark Libraries\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, explode, lit, from_json\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis Globais e de Ambiente para o Projeto.\n",
    "\n",
    "os.environ[\"MINIO_KEY\"] = \"developer\"\n",
    "os.environ[\"MINIO_SECRET\"] = \"developer01\"\n",
    "os.environ[\"MINIO_ENDPOINT\"] = \"http://minio:9000\"\n",
    "\n",
    "\n",
    "# Na ausência do dbtuils ou do Metastore vou usar o boto para me ajudar\n",
    "# a iteragir com o storage\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url = os.environ.get(\"MINIO_ENDPOINT\"),\n",
    "    aws_access_key_id = os.environ.get(\"MINIO_KEY\"),\n",
    "    aws_secret_access_key = os.environ.get(\"MINIO_SECRET\")\n",
    ")\n",
    "\n",
    "bucket_name = \"bank-databr\"\n",
    "\n",
    "# Paths Data Storage\n",
    "root_path_dir = f\"{bucket_name}\"\n",
    "landing_path_dir = f\"{root_path_dir}/landing/bacen\"\n",
    "bronze_path_dir = f\"{root_path_dir}/bronze\"\n",
    "silver_path_dir = f\"{root_path_dir}/silver\"\n",
    "\n",
    "# Partição da tabela Bronze, Data de referência\n",
    "dt_partition = datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "software.amazon.awssdk#s3 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d7b5d2a5-546a-47ab-b960-0e9401a8634b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound software.amazon.awssdk#s3;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#aws-xml-protocol;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#aws-query-protocol;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#protocol-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#sdk-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#utils;2.26.30 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#endpoints-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-auth-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#identity-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-auth-aws;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#checksums-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#checksums;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#profiles;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#retries-spi;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#retries;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#aws-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#regions;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#json-utils;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#third-party-jackson-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#auth;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#http-auth-aws-eventstream;2.26.30 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#http-auth;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#arns;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#crt-core;2.26.30 in central\n",
      "\tfound software.amazon.awssdk#apache-client;2.26.30 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.17.1 in central\n",
      "\tfound software.amazon.awssdk#netty-nio-client;2.26.30 in central\n",
      "\tfound io.netty#netty-codec-http;4.1.111.Final in central\n",
      "\tfound io.netty#netty-common;4.1.111.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.111.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.111.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.111.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.111.Final in central\n",
      "\tfound io.netty#netty-handler;4.1.111.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.111.Final in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.111.Final in central\n",
      "\tfound io.netty#netty-transport-classes-epoll;4.1.111.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 4371ms :: artifacts dl 230ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.17.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-transport-classes-epoll;4.1.111.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.111.Final from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#apache-client;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#arns;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#auth;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-query-protocol;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-xml-protocol;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#checksums;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#checksums-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#crt-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#endpoints-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth-aws;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth-aws-eventstream;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-auth-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#identity-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#json-utils;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#netty-nio-client;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#profiles;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#protocol-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#regions;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#retries;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#retries-spi;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#s3;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#sdk-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#third-party-jackson-core;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.26.30 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   52  |   0   |   0   |   0   ||   52  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d7b5d2a5-546a-47ab-b960-0e9401a8634b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 52 already retrieved (0kB/85ms)\n",
      "25/01/21 14:55:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"SilverLayer\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ[\"MINIO_ENDPOINT\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"MINIO_KEY\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"MINIO_SECRET\"]) \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "                    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções Utilitárias - Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_table_exists_in_metastore(schema_name:str, table_name: str) -> bool:\n",
    "    \"\"\" Verifica se a tabela já existe no schema indicado dentro do Catalógo \"\"\"\n",
    "\n",
    "    return spark._jsparkSession.catalog() \\\n",
    "                               .tableExists(f\"{schema_name}.{table_name}\")\n",
    "\n",
    "\n",
    "def check_table_exists_by_location(s3_minio_client:boto3.client, destionation_table_path: str) -> bool:\n",
    "    \"\"\" Verifica se a tabela já existe, usando o gerenciador do storage \"\"\"\n",
    "\n",
    "    # Vou adaptar usando o boto3 já que não tenho por aqui nem o dbutils nem um metastore para checkar via tableExists()\n",
    "    \n",
    "    # Extrai o bucket e o prefixo (path)\n",
    "    bucket_name, path = destionation_table_path.replace(\"s3a://\", \"\").split(\"/\", 1)\n",
    "    \n",
    "    try:\n",
    "        # Verifica se o diretório existe no S3 (se o prefixo da pasta tiver objetos)\n",
    "        response = s3_minio_client.list_objects_v2(Bucket=bucket_name, Prefix=path, MaxKeys=1)\n",
    "        return 'Contents' in response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        return False\n",
    "\n",
    "def write_delta_table_by_location(df_input: DataFrame, destionation_table_path: str) -> None:\n",
    "    \"\"\" Escreve a tabela Delta no Storage de forma particionada\n",
    "        Se a tabela já existir realiza upsert na partição indicada\n",
    "        Caso não exista primeiramente cria a tabela no modo 'append'\n",
    "\n",
    "        Args:\n",
    "            df_input: dataframe a ser gravado no storage\n",
    "            destionation_table_path: local de destino no storage\n",
    "    \"\"\"\n",
    "\n",
    "    if check_table_exists_by_location(s3_client, destionation_table_path): \n",
    "\n",
    "        print(f\"Tabela já existe: escrevendo nova partição em:\\n\\t*{destionation_table_path}\")\n",
    "\n",
    "        df_input.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"replaceWhere\", f\"partition = {dt_partition}\") \\\n",
    "                .partitionBy(\"dt_partition\") \\\n",
    "                .save(destionation_table_path)   \n",
    "    else:\n",
    "        \n",
    "        print(f\"Tabela ainda não existe, criando nova tabela em: \\n\\t*{destionation_table_path}\")\n",
    "\n",
    "        #TODO: atualizar para considerar o EvolutionSchema, option(\"mergeSchema\": True)\n",
    "        # No formato atual estou considerando somente como EnforceSchema.\n",
    "\n",
    "        df_input.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .partitionBy(\"dt_partition\") \\\n",
    "                .save(destionation_table_path) \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Pagamentos  Trimestrais\n",
    "\n",
    "Aqui vamos ler a tabela \"as-is\" da bronze e aplicar algumas transformações, tratativas na base de dados final para gravação na Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Data Source Pagamentos Trimestrais: s3a://bank-databr/bronze/b_pagamentos_trimestrais_bc\n",
      "\n",
      "* Schema Original do arquivo origem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " @odata.context         | https://was-p.bcn... \n",
      " value                  | [{2024-09-30, 152... \n",
      " file_path              | s3a://bank-databr... \n",
      " file_name              | data_18_01_2025_1... \n",
      " file_size              | 19142                \n",
      " file_block_start       | 0                    \n",
      " file_block_length      | 19142                \n",
      " file_modification_time | 2025-01-18 20:40:39  \n",
      " ingestion_engine       | python_dlt           \n",
      " dt_partition           | 2025-01-20           \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lendo tabela de Pagamentos Trimestrais da Bronze\n",
    "\n",
    "table_name = \"b_pagamentos_trimestrais_bc\"\n",
    "dt_ref_carga= \"2025-01-20\"\n",
    "# Fixando a partição a ser lida, se fosse algo produtivo, e via orquestrador como Control-M ou Airflow\n",
    "# seria uma variável de ambiente com este valor\n",
    "\n",
    "data_source_file_path = f\"s3a://{bronze_path_dir}/{table_name}\"\n",
    "print(f\"\\t* Data Source Pagamentos Trimestrais: {data_source_file_path}\")\n",
    "# Poderíamos usar também o data_*_.json para capturar todos os arquivos na origem.\n",
    "\n",
    "\n",
    "df_pagamentos_trimestral_bronze = spark.read \\\n",
    "                                       .format(\"delta\") \\\n",
    "                                       .load(data_source_file_path) \\\n",
    "                                       .where(col('dt_partition') == dt_ref_carga)\n",
    "\n",
    "print(\"\\n* Schema Original do arquivo origem\")\n",
    "df_pagamentos_trimestral_bronze.printSchema\n",
    "df_pagamentos_trimestral_bronze.show(n=1, vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datatrimestre: string (nullable = true)\n",
      " |-- quantidadeBoleto: double (nullable = true)\n",
      " |-- quantidadeCartaoCredito: double (nullable = true)\n",
      " |-- quantidadeCartaoDebito: double (nullable = true)\n",
      " |-- quantidadeCartaoPrePago: double (nullable = true)\n",
      " |-- quantidadeCheque: double (nullable = true)\n",
      " |-- quantidadeConvenios: double (nullable = true)\n",
      " |-- quantidadeDOC: double (nullable = true)\n",
      " |-- quantidadeDebitoDireto: double (nullable = true)\n",
      " |-- quantidadePix: double (nullable = true)\n",
      " |-- quantidadeSaques: double (nullable = true)\n",
      " |-- quantidadeTEC: double (nullable = true)\n",
      " |-- quantidadeTED: double (nullable = true)\n",
      " |-- quantidadeTransIntrabancaria: double (nullable = true)\n",
      " |-- valorBoleto: double (nullable = true)\n",
      " |-- valorCartaoCredito: double (nullable = true)\n",
      " |-- valorCartaoDebito: double (nullable = true)\n",
      " |-- valorCartaoPrePago: double (nullable = true)\n",
      " |-- valorCheque: double (nullable = true)\n",
      " |-- valorConvenios: double (nullable = true)\n",
      " |-- valorDOC: double (nullable = true)\n",
      " |-- valorDebitoDireto: double (nullable = true)\n",
      " |-- valorPix: double (nullable = true)\n",
      " |-- valorSaques: double (nullable = true)\n",
      " |-- valorTEC: double (nullable = true)\n",
      " |-- valorTED: double (nullable = true)\n",
      " |-- valorTransIntrabancaria: double (nullable = true)\n",
      " |-- dt_partition: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------------------+----------------------+-----------------------+----------------+-------------------+-------------+----------------------+-------------+----------------+-------------+-------------+----------------------------+-----------+------------------+-----------------+------------------+-----------+--------------+--------+-----------------+----------+-----------+--------+-------------+-----------------------+------------+\n",
      "|datatrimestre|quantidadeBoleto|quantidadeCartaoCredito|quantidadeCartaoDebito|quantidadeCartaoPrePago|quantidadeCheque|quantidadeConvenios|quantidadeDOC|quantidadeDebitoDireto|quantidadePix|quantidadeSaques|quantidadeTEC|quantidadeTED|quantidadeTransIntrabancaria|valorBoleto|valorCartaoCredito|valorCartaoDebito|valorCartaoPrePago|valorCheque|valorConvenios|valorDOC|valorDebitoDireto|  valorPix|valorSaques|valorTEC|     valorTED|valorTransIntrabancaria|dt_partition|\n",
      "+-------------+----------------+-----------------------+----------------------+-----------------------+----------------+-------------------+-------------+----------------------+-------------+----------------+-------------+-------------+----------------------------+-----------+------------------+-----------------+------------------+-----------+--------------+--------+-----------------+----------+-----------+--------+-------------+-----------------------+------------+\n",
      "|   2024-09-30|       1525449.9|             5061154.29|             4121806.1|             3132432.21|        44243.73|          661708.42|          0.0|            4056152.31|1.654642727E7|       664224.81|          0.0|    203394.26|                   771817.52| 2463425.06|         662702.46|        241932.98|          78531.64|   194330.6|     961282.82|     0.0|        676418.19|6975429.47|  526775.87|     0.0|1.118021768E7|             6746301.96|  2025-01-20|\n",
      "|   2024-06-30|      1439119.22|             4879670.45|            4057958.85|             3046202.02|        44981.97|          679409.28|          0.0|            3320559.12|1.535689092E7|       698602.29|          0.0|    203269.58|                   357401.33| 2360557.98|         631806.82|        239101.56|          77386.79|  221553.82|     984002.13|     0.0|        589043.45|6284296.05|  591903.75|     0.0|1.066255573E7|             6190513.44|  2025-01-20|\n",
      "|   2024-03-31|      1422709.26|             4723437.71|            3952365.16|             2774237.04|        46468.88|          726660.81|       662.65|            3238627.21|1.357545559E7|        683041.3|       447.19|    203714.16|                   321909.52| 2248815.45|         607470.96|         235111.6|          73035.61|  190457.25|    1082621.22|   650.8|        568515.99|5429305.43|  522123.87|  925.26|   9651625.56|             5396558.52|  2025-01-20|\n",
      "+-------------+----------------+-----------------------+----------------------+-----------------------+----------------+-------------------+-------------+----------------------+-------------+----------------+-------------+-------------+----------------------------+-----------+------------------+-----------------+------------------+-----------+--------------+--------+-----------------+----------+-----------+--------+-------------+-----------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos transformar a tabela original, explodindo os campos JSON/Struct para novas colunas e filtrando algumas variáveis\n",
    "\n",
    "\n",
    "# Data Manipulation, Transformation\n",
    "\n",
    "# Explodindo o valor da coluna \"value\" em múltiplas colunas\n",
    "\n",
    "df_pagamentos_trimestral_transformed = df_pagamentos_trimestral_bronze.withColumn('value_struct', explode(col(\"value\"))) \\\n",
    "                                                                      .select( col(\"value_struct.*\") ) \\\n",
    "                                                                      .drop(*['@odata.context', 'value']) \\\n",
    "                                                                      .withColumn('dt_partition', lit(dt_ref_carga) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Schema Original do arquivo origem\n",
    "df_pagamentos_trimestral_transformed.printSchema()\n",
    "df_pagamentos_trimestral_transformed.show(n=3, vertical=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality com Great Expectations\n",
    "\n",
    "Vamos usar o Great Expectations para de forma padronizada, validar os intervaloes e valores das coluns em nosso Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datatrimestre',\n",
       " 'quantidadeBoleto',\n",
       " 'quantidadeCartaoCredito',\n",
       " 'quantidadeCartaoDebito',\n",
       " 'quantidadeCartaoPrePago',\n",
       " 'quantidadeCheque',\n",
       " 'quantidadeConvenios',\n",
       " 'quantidadeDOC',\n",
       " 'quantidadeDebitoDireto',\n",
       " 'quantidadePix',\n",
       " 'quantidadeSaques',\n",
       " 'quantidadeTEC',\n",
       " 'quantidadeTED',\n",
       " 'quantidadeTransIntrabancaria',\n",
       " 'valorBoleto',\n",
       " 'valorCartaoCredito',\n",
       " 'valorCartaoDebito',\n",
       " 'valorCartaoPrePago',\n",
       " 'valorCheque',\n",
       " 'valorConvenios',\n",
       " 'valorDOC',\n",
       " 'valorDebitoDireto',\n",
       " 'valorPix',\n",
       " 'valorSaques',\n",
       " 'valorTEC',\n",
       " 'valorTED',\n",
       " 'valorTransIntrabancaria',\n",
       " 'dt_partition']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pagamentos_trimestral_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('datatrimestre', 'string'),\n",
       " ('quantidadeBoleto', 'double'),\n",
       " ('quantidadeCartaoCredito', 'double'),\n",
       " ('quantidadeCartaoDebito', 'double'),\n",
       " ('quantidadeCartaoPrePago', 'double'),\n",
       " ('quantidadeCheque', 'double'),\n",
       " ('quantidadeConvenios', 'double'),\n",
       " ('quantidadeDOC', 'double'),\n",
       " ('quantidadeDebitoDireto', 'double'),\n",
       " ('quantidadePix', 'double'),\n",
       " ('quantidadeSaques', 'double'),\n",
       " ('quantidadeTEC', 'double'),\n",
       " ('quantidadeTED', 'double'),\n",
       " ('quantidadeTransIntrabancaria', 'double'),\n",
       " ('valorBoleto', 'double'),\n",
       " ('valorCartaoCredito', 'double'),\n",
       " ('valorCartaoDebito', 'double'),\n",
       " ('valorCartaoPrePago', 'double'),\n",
       " ('valorCheque', 'double'),\n",
       " ('valorConvenios', 'double'),\n",
       " ('valorDOC', 'double'),\n",
       " ('valorDebitoDireto', 'double'),\n",
       " ('valorPix', 'double'),\n",
       " ('valorSaques', 'double'),\n",
       " ('valorTEC', 'double'),\n",
       " ('valorTED', 'double'),\n",
       " ('valorTransIntrabancaria', 'double'),\n",
       " ('dt_partition', 'string')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pagamentos_trimestral_transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pagamentos_trimestral_transformed.isNull"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
